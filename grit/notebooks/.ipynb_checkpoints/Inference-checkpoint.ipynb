{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4a158a3-f5ce-4f3b-a15a-b141bfd636ee",
   "metadata": {},
   "source": [
    "### Inputs\n",
    "\n",
    "(**Please modify it accordingly**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3bc0ec3-359c-4a8d-8e92-522568d04dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = 'COCO_val2014_000000000772.jpg'\n",
    "vocab_path = 'path_to_annotations/vocab.json'\n",
    "checkpoint = \"path_to_checkpoint/grit_checkpoint_4ds.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675b4503-e167-4778-9693-3f10399eab7b",
   "metadata": {},
   "source": [
    "### Intialize a Hydra Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a503d78-fc23-4fe2-99ee-936c0f8886c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import os\n",
    "from omegaconf import OmegaConf\n",
    "from hydra.core.global_hydra import GlobalHydra\n",
    "from hydra import initialize, initialize_config_module, initialize_config_dir, compose\n",
    "\n",
    "# initialize hydra config\n",
    "GlobalHydra.instance().clear()\n",
    "initialize(config_path=\"../configs/caption\", version_base=None)\n",
    "config = compose(config_name='coco_config.yaml', overrides=[f\"exp.checkpoint={checkpoint}\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8058d63e-5a58-446c-9852-668a3beca7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# model\n",
    "from models.common.attention import MemoryAttention\n",
    "from models.caption.detector import build_detector\n",
    "from models.caption import Transformer, GridFeatureNetwork, CaptionGenerator\n",
    "\n",
    "# dataset\n",
    "from PIL import Image\n",
    "from datasets.caption.field import TextField\n",
    "from datasets.caption.transforms import get_transform\n",
    "from engine.utils import nested_tensor_from_tensor_list\n",
    "\n",
    "device = torch.device(f\"cuda:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5ba91d-45f2-4eff-b4be-d615cd39b110",
   "metadata": {},
   "source": [
    "### Build a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b111134-5375-40d8-b661-f7910ea7c6ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/quang/anaconda3/envs/grit/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model missing: 0\n",
      "model unexpected: 0\n"
     ]
    }
   ],
   "source": [
    "detector = build_detector(config).to(device)\n",
    "\n",
    "grit_net = GridFeatureNetwork(\n",
    "    pad_idx=config.model.pad_idx,\n",
    "    d_in=config.model.grid_feat_dim,\n",
    "    dropout=config.model.dropout,\n",
    "    attn_dropout=config.model.attn_dropout,\n",
    "    attention_module=MemoryAttention,\n",
    "    **config.model.grit_net,\n",
    ")\n",
    "cap_generator = CaptionGenerator(\n",
    "    vocab_size=config.model.vocab_size,\n",
    "    max_len=config.model.max_len,\n",
    "    pad_idx=config.model.pad_idx,\n",
    "    cfg=config.model.cap_generator,\n",
    "    dropout=config.model.dropout,\n",
    "    attn_dropout=config.model.attn_dropout,\n",
    "    **config.model.cap_generator,\n",
    ")\n",
    "\n",
    "model = Transformer(\n",
    "    grit_net,\n",
    "    cap_generator,\n",
    "    detector=detector,\n",
    "    use_gri_feat=config.model.use_gri_feat,\n",
    "    use_reg_feat=config.model.use_reg_feat,\n",
    "    config=config,\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "# load checkpoint\n",
    "if os.path.exists(config.exp.checkpoint):\n",
    "    checkpoint = torch.load(config.exp.checkpoint, map_location='cpu')\n",
    "    missing, unexpected = model.load_state_dict(checkpoint['state_dict'], strict=False)\n",
    "    print(\"model missing:\", len(missing))\n",
    "    print(\"model unexpected:\", len(unexpected))\n",
    "    \n",
    "model.cached_features = False\n",
    "\n",
    "# prepare utils\n",
    "transform = get_transform(config.dataset.transform_cfg)['valid']\n",
    "text_field = TextField(vocab_path=vocab_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80123572-55ae-45b7-a0c5-d6c98630c2f4",
   "metadata": {},
   "source": [
    "### Load and Transform An Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ba6f23e-429c-4145-97a2-e2ea8d17c613",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_image = Image.open(img_path).convert('RGB')\n",
    "image = transform(rgb_image)\n",
    "images = nested_tensor_from_tensor_list([image]).to(device)\n",
    "# rgb_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffd4556-d2f5-4a07-badb-2a204e25c692",
   "metadata": {},
   "source": [
    "### Inference and Decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85ef635b-3789-425b-9a95-a75f0f1e4b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "three sheep standing in the grass near a fence\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    \n",
    "    out, _ = model(images,                   \n",
    "                   seq=None,\n",
    "                   use_beam_search=True,\n",
    "                   max_len=config.model.beam_len,\n",
    "                   eos_idx=config.model.eos_idx,\n",
    "                   beam_size=config.model.beam_size,\n",
    "                   out_size=1,\n",
    "                   return_probs=False,\n",
    "                  )\n",
    "    caption = text_field.decode(out, join_words=True)[0]\n",
    "    print(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5652d5f0-90f8-4a2d-88a3-5613074831fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
